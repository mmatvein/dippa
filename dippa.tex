% Lines starting with a percent sign (%) are comments. LaTeX will
% not process those lines. Similarly, everything after a percent
% sign in a line is considered a comment. To produce a percent sign
% in the output, write \% (backslash followed by the percent sign).
% ==================================================================
% Usage instructions:
% ------------------------------------------------------------------
% The file is heavily commented so that you know what the various
% commands do. Feel free to remove any comments you don't need from
% your own copy. When redistributing the example thesis file, please
% retain all the comments for the benefit of other thesis writers!
% ==================================================================
% Compilation instructions:
% ------------------------------------------------------------------
% Use pdflatex to compile! Input images are expected as PDF files.
% Example compilation:
% ------------------------------------------------------------------
% > pdflatex thesis-example.tex
% > bibtex thesis-example
% > pdflatex thesis-example.tex
% > pdflatex thesis-example.tex
% ------------------------------------------------------------------
% You need to run pdflatex multiple times so that all the cross-references
% are fixed. pdflatex will tell you if you need to re-run it (a warning
% will be issued)
% ------------------------------------------------------------------
% Compilation has been tested to work in ukk.cs.hut.fi and kosh.hut.fi
% - if you have problems of missing .sty -files, then the local LaTeX
% environment does not have all the required packages installed.
% For example, when compiling in vipunen.hut.fi, you get an error that
% tikz.sty is missing - in this case you must either compile somewhere
% else, or you cannot use TikZ graphics in your thesis and must therefore
% remove or comment out the tikz package and all the tikz definitions.
% ------------------------------------------------------------------

% General information
% ==================================================================
% Package documentation:
%
% The comments often refer to package documentation. (Almost) all LaTeX
% packages have documentation accompanying them, so you can read the
% package documentation for further information. When a package 'xxx' is
% installed to your local LaTeX environment (the document compiles
% when you have \usepackage{xxx} and LaTeX does not complain), you can
% find the documentation somewhere in the local LaTeX texmf directory
% hierarchy. In ukk.cs.hut.fi, this is /usr/texlive/2008/texmf-dist,
% and the documentation for the titlesec package (for example) can be
% found at /usr/texlive/2008/texmf-dist/doc/latex/titlesec/titlesec.pdf.
% Most often the documentation is located as a PDF file in
% /usr/texlive/2008/texmf-dist/doc/latex/xxx, where xxx is the package name;
% however, documentation for TikZ is in
% /usr/texlive/2008/texmf-dist/doc/latex/generic/pgf/pgfmanual.pdf
% (this is because TikZ is a front-end for PGF, which is meant to be a
% generic portable graphics format for LaTeX).
% You can try to look for the package manual using the ``find'' shell
% command in Linux machines; the find databases are up-to-date at least
% in ukk.cs.hut.fi. Just type ``find xxx'', where xxx is the package
% name, and you should find a documentation file.
% Note that in some packages, the documentation is in the DVI file
% format. In this case, you can copy the DVI file to your home directory,
% and convert it to PDF with the dvipdfm command (or you can read the
% DVI file directly with a DVI viewer).
%
% If you can't find the documentation for a package, just try Googling
% for ``latex packagename''; most often you can get a direct link to the
% package manual in PDF format.
% ------------------------------------------------------------------


% Document class for the thesis is report
% ------------------------------------------------------------------
% You can change this but do so at your own risk - it may break other things.
% Note that the option pdftext is used for pdflatex; there is no
% pdflatex option.
% ------------------------------------------------------------------
\documentclass[12pt,a4paper,oneside,pdftex]{report}

% The input files (tex files) are encoded with the latin-1 encoding
% (ISO-8859-1 works). Change the latin1-option if you use UTF8
% (at some point LaTeX did not work with UTF8, but I'm not sure
% what the current situation is)
\usepackage[utf8]{inputenc}
% OT1 font encoding seems to work better than T1. Check the rendered
% PDF file to see if the fonts are encoded properly as vectors (instead
% of rendered bitmaps). You can do this by zooming very close to any letter
% - if the letter is shown pixelated, you should change this setting
% (try commenting out the entire line, for example).
\usepackage[OT1]{fontenc}
% The babel package provides hyphenating instructions for LaTeX. Give
% the languages you wish to use in your thesis as options to the babel
% package (as shown below). You can remove any language you are not
% going to use.
% Examples of valid language codes: english (or USenglish), british,
% finnish, swedish; and so on.
\usepackage[finnish,english]{babel}


% Font selection
% ------------------------------------------------------------------
% The default LaTeX font is a very good font for rendering your
% thesis. It is a very professional font, which will always be
% accepted.
% If you, however, wish to spicen up your thesis, you can try out
% these font variants by uncommenting one of the following lines
% (or by finding another font package). The fonts shown here are
% all fonts that you could use in your thesis (not too silly).
% Changing the font causes the layouts to shift a bit; you many
% need to manually adjust some layouts. Check the warning messages
% LaTeX gives you.
% ------------------------------------------------------------------
% To find another font, check out the font catalogue from
% http://www.tug.dk/FontCatalogue/mathfonts.html
% This link points to the list of fonts that support maths, but
% that's a fairly important point for master's theses.
% ------------------------------------------------------------------
% <rant>
% Remember, there is no excuse to use Comic Sans, ever, in any
% situation! (Well, maybe in speech bubbles in comics, but there
% are better options for those too)
% </rant>

% \usepackage{palatino}
% \usepackage{tgpagella}



% Optional packages
% ------------------------------------------------------------------
% Select those packages that you need for your thesis. You may delete
% or comment the rest.

% Natbib allows you to select the format of the bibliography references.
% The first example uses numbered citations:
\usepackage[square,sort&compress,numbers]{natbib}
% The second example uses author-year citations.
% If you use author-year citations, change the bibliography style (below);
% acm style does not work with author-year citations.
% Also, you should use \citet (cite in text) when you wish to refer
% to the author directly (\citet{blaablaa} said blaa blaa), and
% \citep when you wish to refer similarly than with numbered citations
% (It has been said that blaa blaa~\citep{blaablaa}).
% \usepackage[square]{natbib}

% The alltt package provides an all-teletype environment that acts
% like verbatim but you can use LaTeX commands in it. Uncomment if
% you want to use this environment.
% \usepackage{alltt}

% The eurosym package provides a euro symbol. Use with \euro{}
\usepackage{eurosym}

% HILIGHTS
\usepackage{soul}

% Verbatim provides a standard teletype environment that renderes
% the text exactly as written in the tex file. Useful for code
% snippets (although you can also use the listings package to get
% automatic code formatting).
\usepackage{verbatim}

% The listing package provides automatic code formatting utilities
% so that you can copy-paste code examples and have them rendered
% nicely. See the package documentation for details.
% \usepackage{listings}

% The fancuvrb package provides fancier verbatim environments
% (you can, for example, put borders around the verbatim text area
% and so on). See package for details.
% \usepackage{fancyvrb}

% Supertabular provides a tabular environment that can span multiple
% pages.
%\usepackage{supertabular}
% Longtable provides a tabular environment that can span multiple
% pages. This is used in the example acronyms file.
\usepackage{longtable}

% The fancyhdr package allows you to set your the page headers
% manually, and allows you to add separator lines and so on.
% Check the package documentation.
% \usepackage{fancyhdr}

% Subfigure package allows you to use subfigures (i.e. many subfigures
% within one figure environment). These can have different labels and
% they are numbered automatically. Check the package documentation.
\usepackage{subfigure}

% The titlesec package can be used to alter the look of the titles
% of sections, chapters, and so on. This example uses the ``medium''
% package option which sets the titles to a medium size, making them
% a bit smaller than what is the default. You can fine-tune the
% title fonts and sizes by using the package options. See the package
% documentation.
\usepackage[medium]{titlesec}

% The TikZ package allows you to create professional technical figures.
% The learning curve is quite steep, but it is definitely worth it if
% you wish to have really good-looking technical figures.
\usepackage{tikz}
% You also need to specify which TikZ libraries you use
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations.pathmorphing,decorations.markings}
\usetikzlibrary{shapes}
\usetikzlibrary{patterns}


% The aalto-thesis package provides typesetting instructions for the
% standard master's thesis parts (abstracts, front page, and so on)
% Load this package second-to-last, just before the hyperref package.
% Options that you can use:
%   mydraft - renders the thesis in draft mode.
%             Do not use for the final version.
%   doublenumbering - [optional] number the first pages of the thesis
%                     with roman numerals (i, ii, iii, ...); and start
%                     arabic numbering (1, 2, 3, ...) only on the
%                     first page of the first chapter
%   twoinstructors  - changes the title of instructors to plural form
%   twosupervisors  - changes the title of supervisors to plural form
\usepackage[mydraft]{aalto-thesis}
%\usepackage[mydraft,doublenumbering]{aalto-thesis}
%\usepackage{aalto-thesis}


% Hyperref
% ------------------------------------------------------------------
% Hyperref creates links from URLs, for references, and creates a
% TOC in the PDF file.
% This package must be the last one you include, because it has
% compatibility issues with many other packages and it fixes
% those issues when it is loaded.
\RequirePackage[pdftex]{hyperref}
% Setup hyperref so that links are clickable but do not look
% different
\hypersetup{colorlinks=false,raiselinks=false,breaklinks=true}
\hypersetup{pdfborder={0 0 0}}
\hypersetup{bookmarksnumbered=true}
% The following line suggests the PDF reader that it should show the
% first level of bookmarks opened in the hierarchical bookmark view.
\hypersetup{bookmarksopen=true,bookmarksopenlevel=1}
% Hyperref can also set up the PDF metadata fields. These are
% set a bit later on, after the thesis setup.


% Thesis setup
% ==================================================================
% Change these to fit your own thesis.
% \COMMAND always refers to the English version;
% \FCOMMAND refers to the Finnish version; and
% \SCOMMAND refers to the Swedish version.
% You may comment/remove those language variants that you do not use
% (but then you must not include the abstracts for that language)
% ------------------------------------------------------------------
% If you do not find the command for a text that is shown in the cover page or
% in the abstract texts, check the aalto-thesis.sty file and locate the text
% from there.
% All the texts are configured in language-specific blocks (lots of commands
% that look like this: \renewcommand{\ATCITY}{Espoo}.
% You can just fix the texts there. Just remember to check all the language
% variants you use (they are all there in the same place).
% ------------------------------------------------------------------
\newcommand{\TITLE}{The Design and Implementation of a Virtual Reality Toolkit}
\newcommand{\FTITLE}{}
\newcommand{\STITLE}{}
\newcommand{\SUBTITLE}{}
\newcommand{\FSUBTITLE}{}
\newcommand{\SSUBTITLE}{}
\newcommand{\DATE}{}
\newcommand{\FDATE}{}
\newcommand{\SDATE}{}

% Supervisors and instructors
% ------------------------------------------------------------------
% If you have two supervisors, write both names here, separate them with a
% double-backslash (see below for an example)
% Also remember to add the package option ``twosupervisors'' or
% ``twoinstructors'' to the aalto-thesis package so that the titles are in
% plural.
% Example of one supervisor:
%\newcommand{\SUPERVISOR}{Professor Antti Ylä-Jääski}
%\newcommand{\FSUPERVISOR}{Professori Antti Ylä-Jääski}
%\newcommand{\SSUPERVISOR}{Professor Antti Ylä-Jääski}
% Example of twosupervisors:
\newcommand{\SUPERVISOR}{Professor Perttu Hämäläinen}
\newcommand{\FSUPERVISOR}{Professori Perttu Hämäläinen}
\newcommand{\SSUPERVISOR}{Professor Perttu Hämäläinen}

% If you have only one instructor, just write one name here
\newcommand{\INSTRUCTOR}{Tuukka Takala M.Sc. (Tech.)}
\newcommand{\FINSTRUCTOR}{Diplomi-insinööri Tuukka Takala}
%\newcommand{\SINSTRUCTOR}{Diplomingenjör Olli Ohjaaja}
% If you have two instructors, separate them with \\ to create linefeeds
% \newcommand{\INSTRUCTOR}{Olli Ohjaaja M.Sc. (Tech.)\\
%  Elli Opas M.Sc. (Tech)}
%\newcommand{\FINSTRUCTOR}{Diplomi-insinööri Olli Ohjaaja\\
%  Diplomi-insinööri Elli Opas}
%\newcommand{\SINSTRUCTOR}{Diplomingenjör Olli Ohjaaja\\
%  Diplomingenjör Elli Opas}

% If you have two supervisors, it is common to write the schools
% of the supervisors in the cover page. If the following command is defined,
% then the supervisor names shown here are printed in the cover page. Otherwise,
% the supervisor names defined above are used.
%\newcommand{\COVERSUPERVISOR}{}

% The same option is for the instructors, if you have multiple instructors.
% \newcommand{\COVERINSTRUCTOR}{Olli Ohjaaja M.Sc. (Tech.), Aalto University\\
%  Elli Opas M.Sc. (Tech), Aalto SCI}


% Other stuff
% ------------------------------------------------------------------
\newcommand{\PROFESSORSHIP}{}
\newcommand{\FPROFESSORSHIP}{}
\newcommand{\SPROFESSORSHIP}{}
% Professorship code is the same in all languages
\newcommand{\PROFCODE}{}
\newcommand{\KEYWORDS}{}
\newcommand{\FKEYWORDS}{}
\newcommand{\LANGUAGE}{English}
\newcommand{\FLANGUAGE}{Englanti}
\newcommand{\SLANGUAGE}{Engelska}

% Author is the same for all languages
\newcommand{\AUTHOR}{Mikael Matveinen}


% Currently the English versions are used for the PDF file metadata
% Set the PDF title
\hypersetup{pdftitle={\TITLE\ \SUBTITLE}}
% Set the PDF author
\hypersetup{pdfauthor={\AUTHOR}}
% Set the PDF keywords
\hypersetup{pdfkeywords={\KEYWORDS}}
% Set the PDF subject
\hypersetup{pdfsubject={Master's Thesis}}


% Layout settings
% ------------------------------------------------------------------

% When you write in English, you should use the standard LaTeX
% paragraph formatting: paragraphs are indented, and there is no
% space between paragraphs.
% When writing in Finnish, we often use no indentation in the
% beginning of the paragraph, and there is some space between the
% paragraphs.

% If you write your thesis Finnish, uncomment these lines; if
% you write in English, leave these lines commented!
% \setlength{\parindent}{0pt}
% \setlength{\parskip}{1ex}

% Use this to control how much space there is between each line of text.
% 1 is normal (no extra space), 1.3 is about one-half more space, and
% 1.6 is about double line spacing.
% \linespread{1} % This is the default
% \linespread{1.3}

% Bibliography style
% acm style gives you a basic reference style. It works only with numbered
% references.
\bibliographystyle{acm}
% Plainnat is a plain style that works with both numbered and name citations.
% \bibliographystyle{plainnat}


% Extra hyphenation settings
% ------------------------------------------------------------------
% You can list here all the files that are not hyphenated correctly.
% You can provide many \hyphenation commands and/or separate each word
% with a space inside a single command. Put hyphens in the places where
% a word can be hyphenated.
% Note that (by default) LaTeX will not hyphenate words that already
% have a hyphen in them (for example, if you write ``structure-modification
% operation'', the word structure-modification will never be hyphenated).
% You need a special package to hyphenate those words.
\hyphenation{di-gi-taa-li-sta yksi-suun-tai-sta}



% The preamble ends here, and the document begins.
% Place all formatting commands and such before this line.
% ------------------------------------------------------------------
\begin{document}
% This command adds a PDF bookmark to the cover page. You may leave
% it out if you don't like it...
\pdfbookmark[0]{Cover page}{bookmark.0.cover}
% This command is defined in aalto-thesis.sty. It controls the page
% numbering based on whether the doublenumbering option is specified
\startcoverpage

% Cover page
% ------------------------------------------------------------------
% Options: finnish, english, and swedish
% These control in which language the cover-page information is shown
\coverpage{english}


% Abstracts
% ------------------------------------------------------------------
% Include an abstract in the language that the thesis is written in,
% and if your native language is Finnish or Swedish, one in that language.

% Abstract in English
% ------------------------------------------------------------------
\thesisabstract{english}{
The introduction of game consoles with motion tracked controllers and depth sensing cameras has initiated a new wave of movement-based applications and games. These input devices paired with 3D display technologies that have recently become mainstream give hobbyists a chance of participating in the creation of virtual reality software at a much lower price point than was previously possible.

This thesis details the design and implementation of RUIS (Reality-based User Interface System), a virtual reality toolkit for Unity3D. The main goal of RUIS is to make the creation of VR applications as straightforward as possible. This is achieved by providing users with basic building blocks that solve many of the challenges encountered in the development of VR software. These building blocks provide a layer of abstraction which gives the developer the chance to focus on interaction methods and the application itself.

The two main tasks RUIS handles are input and display configuration management. The challenges involved in the management of input methods are for example the calibraton of coordinate systems, providing a clean interface for the usage of different types of devices in a standarized way and providing prebuilt modules that handle common VR tasks such as head tracking. The multitude of different virtual reality display configurations on the other hand require solutions for multi-display environments, head tracked views, projector keystoning correction and stereoscopic displays. RUIS implements these features and gives the developer a chance to create motion-based 3D applications easily inside the Unity Editor. 

Special attention during development was also given to the usability of RUIS - especially the idiomatic usage of Unity3D. This emphasis was put to the test on the Aalto University course T-111.5400 Virtual Reality, where the students used RUIS to create their applications. This thesis also discusses the results and lessons learned during the course. }


%A dissertation or thesis is a document submitted in support of candidature
%for a degree or professional qualification presenting the author's research and
%findings. In some countries/universities, the word thesis or a cognate is used
%as part of a bachelor's or master's course, while dissertation is normally
%applied to a doctorate, whilst, in others, the reverse is true.

%\fixme{Abstract text goes here (and this is an example how to use fixme).}
%Fixme is a command that helps you identify parts of your thesis that still
%require some work. When compiled in the custom \texttt{mydraft} mode, text
%parts tagged with fixmes are shown in bold and with fixme tags around them. When
%compiled in normal mode, the fixme-tagged text is shown normally (without
%special formatting). The draft mode also causes the ``Draft'' text to appear on
%the front page, alongside with the document compilation date. The custom
%\texttt{mydraft} mode is selected by the \texttt{mydraft} option given for the
%package \texttt{aalto-thesis}, near the top of the \texttt{thesis-example.tex}
%file.

%The thesis example file (\texttt{thesis-example.tex}), all the chapter content
%files (\texttt{1introduction.tex} and so on), and the Aalto style file
%(\texttt{aalto-thesis.sty}) are commented with explanations on how the Aalto
%thesis works. The files also contain some examples on how to customize various
%details of the thesis layout, and of course the example text works as an
%example in itself. Please read the comments and the example text; that should
%get you well on your way!

% Abstract in Finnish
% ------------------------------------------------------------------
\thesisabstract{finnish}{

}

% Abstract in Swedish
% ------------------------------------------------------------------
%\thesisabstract{swedish}{
%}


% Acknowledgements
% ------------------------------------------------------------------
% Select the language you use in your acknowledgements
\selectlanguage{english}

% Uncomment this line if you wish acknoledgements to appear in the
% table of contents
%\addcontentsline{toc}{chapter}{Acknowledgements}

% The star means that the chapter isn't numbered and does not
% show up in the TOC
\chapter*{Acknowledgements}

\vskip 10mm

\noindent Espoo, \DATE
\vskip 5mm
\noindent\AUTHOR

% Acronyms
% ------------------------------------------------------------------
% Use \cleardoublepage so that IF two-sided printing is used
% (which is not often for masters theses), then the pages will still
% start correctly on the right-hand side.
\cleardoublepage
% Example acronyms are placed in a separate file, acronyms.tex
% \input{acronyms}

\addcontentsline{toc}{chapter}{Abbreviations and Acronyms}
\chapter*{Abbreviations and Acronyms}

% The longtable environment should break the table properly to multiple pages,
% if needed

\noindent
\begin{longtable}{@{}p{0.25\textwidth}p{0.7\textwidth}@{}}
CAVE & Cave Automatic Virtual Environment \\
DOF & Degrees of freedom \\
HMD & Head-Mounted Display
\end{longtable}


% Table of contents
% ------------------------------------------------------------------
\cleardoublepage
% This command adds a PDF bookmark that links to the contents.
% You can use \addcontentsline{} as well, but that also adds contents
% entry to the table of contents, which is kind of redundant.
% The text ``Contents'' is shown in the PDF bookmark.
\pdfbookmark[0]{Contents}{bookmark.0.contents}
\tableofcontents

% List of tables
% ------------------------------------------------------------------
% You only need a list of tables for your thesis if you have very
% many tables. If you do, uncomment the following two lines.
% \cleardoublepage
% \listoftables

% Table of figures
% ------------------------------------------------------------------
% You only need a list of figures for your thesis if you have very
% many figures. If you do, uncomment the following two lines.
% \cleardoublepage
% \listoffigures

% The following label is used for counting the prelude pages
\label{pages-prelude}
\cleardoublepage

%%%%%%%%%%%%%%%%% The main content starts here %%%%%%%%%%%%%%%%%%%%%
% ------------------------------------------------------------------
% This command is defined in aalto-thesis.sty. It controls the page
% numbering based on whether the doublenumbering option is specified
\startfirstchapter

% Add headings to pages (the chapter title is shown)
\pagestyle{headings}

% The contents of the thesis are separated to their own files.
% Edit the content in these files, rename them as necessary.
% ------------------------------------------------------------------

% \input{1introduction.tex}

\chapter{Introduction}
\label{chapter:intro}


% \input{2background.tex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Background}
\label{chapter:background}

This chapter briefly discusses the history of virtual reality and how it has come to the stage where a toolkit like RUIS makes it feasible for hobbyists to get involved.

\section{Brief History of Virtual Reality}
\label{section:historyofvr}

\section{Display environments}
\label{section:displayenvironments}

\subsection{Stereoscopic Displays and Projectors}
\label{subsection:displayenvironments:stereoscopic}


\subsection{Cave Automatic Virtual Environment (CAVE)}
\label{subsection:displayenvironments:cave}

\subsection{Head-mounted Displays}
\label{subsection:displayenvironments:hmd}

\section{Motion controllers}
\label{section:motioncontrollers}

\hl{some examples of older motion tracking systems}


\subsection{Depth-sensing Cameras}
\label{subsection:motion:kinect}

The Microsoft Kinect is a depth-sensing camera originally launched in November 2010. \hl{SOURCE}

\subsection{Wand Controllers}
\label{subsection:motion:move}

Wand controllers are input devices that are held in the hand like a wand. These devices can be used to point at virtual objects and manipulate them. Wand controllers usually include buttons at convenient locations to make for example grabbing onto objects quite natural. Wands usually offer either 3 or 6 degrees of freedom (DOF) tracking, meaning that either only the orientation or the orientation and position of the controller are tracked, respectively. 6 DOF controllers usually require a separate camera or other type of sensor to track the position of the wand, where as 3 DOF controllers can work independently.

The first mainstream wand controller was the Nintendo Wii Remote \cite{WiiRemoteMain}, offering 3 degrees of freedom. Wii Remotes use an accelerometer to track the orientation of the controller. The use of only an accelerometer introduces drift in the yaw direction, so the tracking system also includes an infrared transmitter bar. This transmitter bar is seen by a camera at the tip of the Wii Remote controller as two infrared dots. This information combined with the rest of the orientation data of the controller can be combined to eliminate yaw drift and get accurate rotational tracking. Conversely, when the infrared transmitter bar is not seen by the controller, i.e. the controller is pointed away from the screen, rotation tracking accuracy will deteriorate. The Wii Remote was augmented later on with the Wii MotionPlus add-on, which includes a gyroscope, improving the rotational tracking considerably. 

Sony's PlayStation Move \cite{PSMoveMain} quickly followed the Wii Remote. The Move controller features 6 DOF tracking, which makes it more usable in virtual reality applications requiring precise positional data. The positional tracking is achieved by having a separate camera track a glowing orb at the end of the controller. This kind of optical tracking results in the system being very vulnerable to occlusion issues. The user cannot for example hold the controller behind his body without losing accuracy. The range of the camera is also quite small, so the user does not have that big of an area to move in. \hl{more stuff}

The Razer Hydra is, like the PS Move, a 6 DOF controller. The Hydra, however, uses magnetic tracking, which removes the occlusion problems of the camera-based tracking in the Move. The system consists of two wired controllers that are connected to a magnetic central unit. Magnetic tracking has a shorter range, though, and since the controllers are wired this range is enforced very concretely. This makes the hydra best suited for use while sitting at a desk. When testing RUIS we mounted the magnetic unit onto the users torso in order to make the Hydra more mobile. This works quite well as long as the mount is stable. 

\section{Similar toolkits}
\label{section:similar}

\hl{RUIS for Processing}

\hl{MiddleVR}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Design}
\label{chapter:design}

This chapter details the design considerations in the development of RUIS.

\section{Unity3D}
\label{section:unity3d}

Unity3D \cite{UnitySite} is a game development environment and an engine that has recently gained a lot of popularity because of its power and ease of use. Unity3D simplifies common game development and content creation tasks, shortening development time.

Code architecture in Unity3D is very much based on a common base class for all scene objects called GameObject. Functionality can be added to these GameObjects by attaching Components; either the ones that are built into Unity3D or by creating your own scripts inheriting the class MonoBehaviour. This way, different types of objects in the scene can be composed of small modular components with minimal coupling, keeping the overall structure quite easy to follow.

Unity3D also features prefabs: prefabricated objects that can be drag and dropped into a scene. This allows users to create pre-built common game objects and easily share them between scenes. The use of prefabs is an important part of using Unity3D efficiently, so it was a big point to take into consideration in the development of RUIS. 

\section{System Architechture}
\label{section:systemarchitecture}

\subsection{Folder Structure}
\label{subsection:systemarchitecture:folderstructure}

\subsection{Prefabs}
\label{subsection:systemarchitecture:prefabs}

\subsection{Custom Editors}
\label{subsection:systemarchitecture:customeditors}

\section{External Libraries}
\label{section:externallibraries}

Several external libraries and frameworks were used in the development of RUIS, especially to interface with all the different input devices that are supported. 

\subsection{OpenNI}
\label{subsection:external:openni}

RUIS uses OpenNI \hl{OpenNI site} as its skeletal tracking library. OpenNI is an open-source framework used for developing applications featuring natural bodily interaction. In the case of RUIS, OpenNI is used in tandem with the NiTE middleware libraries \hl{NiTE site}. NiTE extracts the skeletal data and RUIS references it through the OpenNI Unity Wrapper. 

\hl{more, primesense drivers for xtion and kinect etc.}

\subsection{Oculus SDK}
\label{subsection:external:oculussdk}

\subsection{Other}
\label{subsection:external:other}

RUIS uses some additional libraries to interface with input systems. To communicate with the Move.Me server we integrated PSMoveWrapper \cite{PSMoveWrapper}, with some minor modifications. Most of the functionality in PSMoveWrapper is further wrapped by separate RUIS components. For Razer Hydra \cite{RazerHydra}, RUIS uses the Sixense Unity Plug-in \cite{RazerUnityPlugin}. Finally, RUIS uses CSML, the C Sharp Matrix Library for basic matrix manipulations and calculations. It is used for example to calculate the transformation matrix between the Kinect and PS Move coordinate systems.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Display Configuration Management}
\label{chapter:displayconfigurationmanagement}

This chapter details all the implemented features regarding the management of displays. For starters, it is important for the user to be able to switch easily between monoscopic and stereoscopic modes of display. In addition to this, multiple displays of possibly different resolutions should be supported in a straightforward manner.

When displaying virtual worlds on a static screen relative to the movement of the observing user, the stereoscopic effect and general immersion level can be improved by modifying the perspective projection matrix to account for the changes in head position. This can make it seem as if the user were looking at the virtual world through a real window. This method requires accurate head tracking in order to be fully immersive.

As the last step in many projector systems, a projector keystoning correction will have to be applied. Projector keystoning is caused by projectors being off-center relative to the plane they are projecting on. This results in a distorted image that will be especially disturbing in multi-projector stereoscopic environments: The images for both eyes will not match and discrete seams will appear between the images meant for separate walls.

Finally, this chapter discusses the challenges we faced when integrating the Oculus SDK into our project. A head-mounted display requires some extra configurability for example in the head tracking settings. We also wanted to make sure this freedom did not come at the expense of usability.

\section{Multi-Display Systems and Stereoscopy}
\label{section:multidisplaysystems}

Many virtual reality applications make use of more than one display. Separate displays can be used to either show completely different information, or to wrap the virtual world around the user. Especially the latter is common in CAVE-like environments.

The way Unity3D handles multiple camera viewpoints is by requiring the application to specify the normalized viewport rectangle of each camera, in other words, the area of the application window that the camera will render to. While the user can do this manually for each camera, the process will become tedious very quickly, especially when configuring multiple stereoscopic displays. 

We tackle this problem by introducing a display manager component, RUISDisplayManager, to handle the bulk of display-related configuration issues. The user can add, remove and rearrange displays via the editor. Separate displays are called RUISDisplays, signifying their connection to RUIS. Each RUISDisplay works independently of the other displays - the display manager handles all shared tasks. Each RUISDisplay is then linked to a RUISCamera - RUIS's version of the Unity3D standard Camera. This chain of components implements all basic tasks required for multi-display, mono- and/or stereoscopic applications.

\subsection{Multiple Displays}
\label{subsection:multidisplaysystems:multipledisplays}

For each display, the user is able to specify the resolution of the display. The resolution specified is used to calculate the total needed resolution of the application. After this, each display, and the camera linked to it, is assigned its normalized viewport rectangle. 

\hl{could have example pic of different display configurations}

\subsection{Stereoscopy}
\label{subsection:multidisplaysystems:stereoscopy}

Once the basic framework for multiple displays is in place, it is quite straightforward to extend its functionality to include switching between mono and stereo displays. Most of the functionality for this is implemented at the RUISCamera level.

In addition to the Unity3D Camera attached to the RUISCamera, two new GameObjects with the Camera script will have to be added underneath it: the left and right eye cameras. The Camera script attached to the parent GameObject is treated as the monoscopic camera, while the left and right cameras are used for stereoscopic rendering. The RUISCamera script will then reference all these three Camera scripts and enable them based on whether the display rendered to is mono or stereo. 

The most straightforward way to assign the left and right cameras the correct viewport rectangle is to just halve the rectangle of the center camera. The user makes a decision in which direction he wants to separate the eyes, side-by-side or top-and-bottom, based on the settings of the display device. After disabling the center camera and enabling the left and right cameras both images will render correctly in order for the display device to display a stereoscopic image.

The last part in setting up a correct stereoscopic view is configuring the cameras to be at the correct positions. The eye separation, or interpupillary distance (IPD), will have to be set to a value that suits the user. Dodgson  \cite{Dodgson04variationand} has found the IPD of adults in general to be between 52 mm and 78 mm, with the mean being 63 mm. This variability makes it important for the value to be configurable. RUIS handles IPD configurability by offsetting the left and right camera GameObjects from the center camera. By default, these cameras are then set to look straight ahead. The user can also tweak the so-called zero parallax distance - the distance where the left and right camera view rays converge.

\section{Perspective Projection for a Head-Tracked View}
\label{section:perspectiveprojection}

In CAVE and non-HMD systems a big part of the depth effect and positional awareness comes from moving the head in relation to the display. If the image on the screen stays static, the effect is non-existent and the screen will look like a picture frame. By adding head tracking and shifting the viewpoint with the head, a new level of depth is achieved. Even this results in a relatively flat image, though. To take the effect even further, the projection matrix used for the cameras can be modified to make the display or projector screen seem like a window to the virtual world. This effect is demonstrated well in Lee's \cite{JohnnyLeePerspective} video on the subject. The projection matrix has to be modified so that the frustum edges from each eye match the corresponding display edges. This requires knowledge of both the eye locations as well as the display location in tracker coordinates.

\hl{image of frustum getting skewed}

\hl{start writing about stuff in the paper about head-tracked views}

\section{Projector Keystoning Correction}
\label{section:keystonecorrection}

\cite{TUGrazKeystoning}

\section{Integrating Oculus Rift}
\label{section:integratingoculusrift}

Separate from all of the previous functinality is the Oculus Rift. Applications developed for the Rift require very different features from the display management system than CAVE applications, for example. Applications designed for head-mounted displays (HMD) usually use the HMD as their only display. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Input Management}
\label{chapter:inputmanagement}

The second big subsystem of RUIS is the input manager. It takes care of all the chores related to using the multitude of input systems RUIS uses, leaving only the implementation of application specific functionality to the user.

\section{Coordinate Systems}
\label{section:coordinatesystem}

All of the motion control devices supported by RUIS operate in their own coordinate systems. In order to use all of these devices together the coordinate systems will have to be calibrated into one common system. Unity3D uses a consistent left-handed coordinate system with the basic distance unit being one meter, therefore, to make the use of RUIS as easy as possible for the end-user, it makes sense to transform all the input coordinates into the Unity3D coordinates.

\subsection{Coordinate System Calibration}
\label{subsection:coordinatesystems:calibration}

The coordinate system calibration process is implemented in its own standalone scene. The user can start this procedure via the RUIS Menu. 

In order to combine Move and Kinect coordinates samples are taken from both systems, after which a transformation matrix is calculated. This matrix transforms the Move coordinates into their Kinect counterparts. The matrix includes rotation and translation transforms. Scale is excluded since the scaling difference between the two coordinate systems is already known beforehand: Kinect uses millimeters, while Move uses centimeters.

The Move to Kinect transformation matrix is calculated by creating the equation \ref{eq:m2kmatrix}, where K and M are n by 3 matrices, containing n corresponding Kinect and Move samples, and A is the wanted transform. This linear matrix equation can then be solved straightforwardly.

\begin{equation}
    K = AM
    \label{eq:m2kmatrix}
\end{equation}

After finding the matrix A, the rotation and translation components are separated into a Quaternion and Vector3, which are ready to be applied to Move orientations and positions to turn them into their Kinect counterparts.

Also, the real-world floor location and normal are found during the calibration process by using OpenNI's scene analysis tools. This information is then used to tilt the coordinate system to counteract the tilt angle of the Kinect camera and to set the floor to be at the y = 0 plane if the user wishes to do so.

\subsection{Controlling Virtual Characters}
\label{subsection:coordinatesystems:virtualcharacters}

When moving virtual characters it is important to differentiate which coordinate system the character will be moved in. In addition to the local and global coordinates of the game object itself, we have to take tracker coordinates into account. For example when calculating projection matrixes for head tracked views, the matrix is calculated using the eyes' positions relative to the real-world coordinates of the screen. 

\hl{avaruusalusmalli, ruislocomotion, pivot point hahmoa kaantaessa}

\section{Skeleton Tracking and Character Animation}
\label{section:skeletontracking}

To display a virtual character that matches the real-world movements of the user, the user skeleton will have to be tracked. RUIS uses OpenNI and a depth-sensing camera to achieve this. This procedure is detailed in chapter \ref{subsection:external:openni}. This section focuses on the software building upon the basic skeletal tracking to enable the creation of rich, interactive applications. 

\subsection{Skeleton Management}
\label{subsection:skeletontracking:skeletonmanagement}

\subsection{Using Premade Models and Armatures}
\label{subsection:skeletontracking:premademodels}

Supporting the whole plethora of user-made humanoid models available is a big undertaking. There is no set-in-stone standard that all armatures adhere to, so in the implementation of RUIS we make certain assumptions of the structure of the model.

Armatures are tree-based bone structures that control the deformation of the model mesh. Each mesh vertex has a number of bone weights, which determine the bones that affect the position of the vertex and by which strenghts. To move different parts of the model, the armature bones will therefore have to be controlled. To control a model using Kinect, we created a component called RUISSkeletonController, which takes in skeleton data and uses it to control bones in a model. Basically RUISSkeletonController is just a link that copies and translates transforms from one place, RUISSkeletonManager, to the other, the armature.

This approach, however, works directly only for non-hierarchical armatures, since the data within RUISSkeletonManager is non-hierarchical. RUISSkeletonController therefore translates these positions and rotations into a hierarchical version by making the assumption that the structure of the armature follows the structure outlined in figure \ref{fig:skeletontreestructure}. All rotations are set to be correct in the global coordinate system, when compared to the skeleton torso joint. For hierarchical models, the local positions of bones will not have to be set, since transformations propagate through the armature. On the other hand, in custom made models and armatures the bone lengths will most likely not match the real users' bone lengths, so the joint positions will be incorrect. For this reason, the bones will have to be scaled to match their real counterparts.

\hl{skeletontreestructure figure}
\begin{figure}
    \label{fig:skeletontreestructure}
\end{figure}

Scaling the armature bones to the correct lengths is a relatively straightforward process: For each bone to be scaled, a multiplier is applied to the local scale of the bone to make it the correct size. In hierarchical models these local scales will also scale the children of the bone, so this effect will have to be negated in the scales of the children. Making the same structure assumption as in the previous phase, the easiest way to achieve the correct armature scale is to first scale the torso to the correct size. After this each limb is scaled separately by starting at the root of the limb. \hl{torso scaling process, width between hip joints as correct scale} An example of the final calculation for the scale of the forearm is calculated as in \ref{eq:forearmscale}. 

\hl{make prettier}
\begin{equation}
    \label{eq:forearmscale}
    SoF_{new} = \frac{SoF_{user}}{SoF_{original}} * \frac{1}{scale_{torso} * scale_{upperarm}}
\end{equation}

Even the scaling process does not always fix all the joints to be in the exact same positions as the user's joints. Small changes in the armature setup, such as the armature's shoulders being made of two bones, a clavicle bone and an upper arm bone, instead of just one like in the Kinect skeleton. \hl{neck height, forearm length, scaling hands}

As a final touch, everything can be smoothed out by applying some damping to the raw rotations given by the skeleton manager. The OpenNI system applies some filtering to the skeleton, but this is oftentimes not enough to get rid of all the jitter. We have therefore implemented simple clamping of the maximum change per second for rotations and scales.

\subsection{Combining Kinect and Mecanim}
\label{subsection:skeletontracking:kinectandmecanim}

In many cases the application developer might want to add some pre-made animation to the avatar to make the world seem more vibrant. This can range from simple walk cycles to complex death animations. Unfortunately Unity3D's Mecanim system does not have its own system for blending procedural animation, i.e. our Kinect data, with animations made in animation software. Therefore we have had to bridge this gap by creating our own animation blender that mixes these animations together based on the user preferences. 

Mecanim, introduced in Unity 4, is a state-machine based animation system that lets the developer mix and match animations attached to an avatar by controlling custom parameters. In the most simple case this can be done just by creating a state machine with one state, for example an idle animation. This system can then be built upon by adding different animation layers for the lower and upper body, separate animations for walking in different directions, etc. The blend factors of all of these animations can then be controlled by using the parameters fed to the animator system. 

The approach we take in RUIS is to leave the Mecanim process alone and only use the end result as something to blend with the animations coming from Kinect. This way, the user can create a complex system of hard-coded animations, on top of which he can add Kinect animations whenever he wants to, by controlling the ratio in which Mecanim and Kinect are blended.

Unfortunately, there is no easy way to access the final animation data produced by the whole Mecanim state machine, without having the armatures themselves be animated in the project. To combine the animations into one blended animation we therefore create two copies of the original model: one for Kinect and one for Mecanim. The original GameObject is then used for the final blend. 

\hl{schematic of the skeleton gameobject being divided into three parts}

To blend the bones of the two animations, a three-way connection has to be made that connects the corresponding bones in the two animations with the end result bone. We do this by iterating through the bone hierarchies and matching bones one at a time based on their position in the hierarchy and name. Because all the armatures are created from the same original, this is a relatively safe process. The only drawback is that bones on the same hierarchy level will have to have different names, which is a sound requirement in most well-made armatures anyways. 

During this matching process we also classify the bones to their corresponding body part. This is done to enable the user to control blend weights for body parts separately. The classification process uses the information given in RUISSkeletonController to figure out the roots of each limb, after which it classifies all bones underneath this root as being a part of that limb. 

\hl{blend process itself, setting arm & leg positions the hard way}

\subsection{Adding Physics to a Kinect-Controlled Virtual Character}
\label{subsection:skeletontracking:physics}

Combining real-world physics with virtual physics requires some special consideration. Without special hardware, nothing that happens physically in the virtual world can affect how the user behaves in the real world, while real-world physics are always present in kinect-controlled motion: The floor and objects restrict movement, gravity affects the user's behavior, etc. Therefore, it is important to specify the wanted physical relationship for each application on a case-by-case basis. Also, in order to work around the limitations that the Kinect tracking imposes on the physical interaction, some precautions will have to be taken.

In a perfect world the Kinect-controlled character would be able to interact with the virtual world physically just by adding colliders to every body part of the model. The most rudimentary versions of the RUISSkeletonController enabled prefabs do exactly this. The avatar is a kinematic Unity3D Rigidbody that interacts with the environment but the environment does not affect the way the character moves. This is pretty intuitive behavior, since this way the avatar always matches your real body. The problem with this approach becomes apparent though, when we want the character to for example respect boundaries such as walls, follow the contours of uneven terrain or even fall down from cliffs.

To achieve such behavior, the avatar Rigidbody will have to be made non-kinematic. This opens up a whole new world of issues to deal with, though, since now the environment affects character movements directly. This means that when a large object hits the player avatar, the avatar will start moving on its own. Most of the time, this is unwanted behavior that will have to be circumvented somehow.

One thing we do is to disable all rotations on the x- and z-axes. This way the player avatar will stand up, instead of just falling to the most natural position of a capsule - on its side. The Rigidbody is also made heavier, in order to minimize the effects of other Rigidbody objects on it.

\hl{stabilizing collider}

The data received from the skeleton tracking system is very noisy, which means that any real-time game physics system cannot use this data without experiencing some kind of jitter and unexpected behavior. The most straightforward way to lessen the impact of noise in this situation is to dampen changes between input frames. Movements will become more smooth, but on the other hand they will be even more unresponsive. With the inherent lag of the whole Kinect processing chain, one has to be very wary about introducing even more delay into the motion.

\subsection{Tracking Gestures}
\label{subsection:skeletontracking:gestures}

Bodily interaction methods with no physical buttons to press often require some other way to emulate these types of triggered events. One way to do this is to track the body for gestures, certain predefined movements that signal the intent. For RUIS we implemented a simple hold gesture and a jump gesture, along with the necessary framework for users to be able to create their own more complex ones.

As the basis for a gesture system, we provide an interface, which every gesture should implement. For the most barebones interface, only a function to check whether the gesture was triggered would be needed, but we add some functions to ease the use of gestures, such as functions to enable and disable the gesture. For some types of gestures it can be useful to know how far the gesture has progressed, which is why we also specify a GetGestureProgress function in our interface. By implementing this interface, anything can pose as a RUIS gesture - even complex button combinations can be made into a gesture. 

For the user to be able to easily create gestures, we have created a simple point tracker that tracks a transform on a frame-to-frame basis and calculates statistics about its movement. Currently it only calculates average speed, maximum velocity and average velocity. For some cases it might be good to get the average speed, non-directional velocity, instead of the average velocity, which is why we have separate values. This point tracker is made to be easily extendable to capture for example angular velocity. Gestures can use these point trackers to check for relevant information.

For object selection using Kinect we wanted to create a simple gesture that would test our gesture system and also provide a good example for users to build their gestures upon. The hold gesture is very straightforward to implement using this system. In RUIS, we create the gesture by checking whether the average speed of the tracked point, in this case a point tracker attached to the hand, is below a certain threshold for a certain amount of time. Once the time requirement is fulfilled, the gesture will be triggered. Gesture progress is calculated by comparing the current hold length with the required total time. 

When creating our final demo, detailed in section \ref{section:turbotuscany}, we also needed a gesture to signal that the character should jump. The most obvious choice to implement this is a jump gesture. At first, we tried to implement the gesture by tracking only the center of mass. Our plan was to detect the whole jump movement, which consists of a small dip and an explosive movement upwards, instead of just the upwards movement. By using this kind of gesture detection we were able to catch a major part of the gestures, but this solution still resulted in a lot of missed activations and false positives. The user can for example be crouched, after which jump dynamics are very different. Our second version, the final one now in use, works better by still using the center of mass to track the upwards movement, dropping the requirement of the downwards dip. In addition to this upwards velocity requirement we add the condition that both feet should be off the ground, above some threshold. In practice this last requirement helped get rid of the false positives, while dropping off the downwards movement from the movement recognition helped detect gestures more easily.

For both the hold and jump gestures, an important factor to take into consideration is to allow enough configurability, so that the user can tweak parameters to mold the gestures to work as well as possible in the application. In different applications a different hold length might for example be appropriate. This is something we took into account from the beginning, since Unity3D's user interface encourages exposing variables for editing directly inside the editor.

\hl{something about the hold gesture visualizer?}

\section{Wand Controllers}
\label{section:wandcontrollers}

\subsection{PlayStation Move}
\label{subsection:wandcontrollers:psmove}

The main wand controller RUIS has used from the beginning is the PS Move. It provides accurate tracking with a relatively low latency. To use Move controllers, however, is not as straightforward as it could be, because of Sony's limitations on the tracking software. Currently, the only way to properly use the full functionality of the PS Move controllers is to use a PlayStation 3 console and its Move.me application. This application then acts as a server to which applications can connect via TCP and UDP. Some attempts have been made to create open source tracking software for the PC, but Sony's software's sophistication has made it very hard to replicate all the functionality accurately.

To connect to the Move.me server we use PSMoveWrapper \cite{PSMoveWrapper}, a networking library that implements all the necessary network functionality to use the full features of the Move controllers. On top of this library we created the class RUISPSMoveWand, inheriting from RUISWand, to encapsulate all the relevant features. This way we can for example control which coordinate system the positional data is given in.

One difficulty we faced with the version of PSMoveWrapper we used was that the WasPressed and WasReleased functions, used to check whether a button has been pressed since the last frame, only worked for the first call made to them during an update cycle. This is unwanted behavior and would result in many hard-to-find bugs for users - they would have to read the PSMoveWrapper source code to find out what is happening. We fixed this problem by restructuring the functions so that they return correct values for the whole update cycle.

\subsection{Razer Hydra}
\label{subsection:wandcontrollers:hydra}

\subsection{Skeleton Wand}
\label{subsection:wandcontrollers:skeleton}

By using the gesture system outlined in \ref{subsection:skeletontracking:gestures}, we created a simple way for the user to point at and select objects by using Kinect only. The skeleton wand emulates wand functionality by creating the wand pointer ray from the user's forearm and using a hold gesture to track selection. 

\subsection{Selecting Objects}
\label{subsection:wandcontrollers:selection}

One of the most basic ways to interact with the virtual environment with a wand controller is to use it to point at objects and select them. In RUIS we implement this functionality by introducing two components, RUISWandSelector and RUISSelectable, which are attached to the wand and the object to be selected, respectively. As an overview, the wand selector component constantly casts rays to check whether a selectable object is in front of the wand selection ray. If the user then presses the selection button, or signals the intent to select in another way, the object is selected and it starts following the wand's movements. First, I will discuss the necessary groundwork this requires on the wand's side, after which I will go into more detail regarding the behavior of the selected object.

RUISWand provides a common interface for RUISWandSelector to use no matter which type of wand implements the functionality itself. This way, we can leave the implementation details to the wands themselves, while focusing on the selection process itself. The two main pieces of information needed for selection are the wand transform and the selection button state. Both of these are provided by RUISWand.

Each Update, the RUISWandSelector script checks whether something is in front of its selection ray using one of two methods for creating the ray. The first ray creation method is just to shoot a ray from the tip of the wand to the direction it points at. Using this method, the ray is just created with the transform of the wand. The second method requires the head position in addition to the wand transform: The selection ray is formed from the line between the head and wand positions. This type of wand selection is useful in use cases that for example use movements close to archery.

Once we have created the selection ray, we use Unity's Physics class to raycast to look for colliders that intersect the selection ray. The first applicable collider, meaning that it isn't on one of the layers that should be ignored, is then returned for further processing. At the raycast stage no difference is made between objects that have the RUISSelectable script attached and objects that don't. This way we achieve the natural functionality that for example walls block selection rays - you cannot select something you cannot see.

The selection candidate object returned by the raycast is then searched for the RUISSelectable script, to ascertain whether it is selectable or not. This process comes with a caveat, though, since selectable objects can be comprised of multiple colliders in a collider hierarchy. Unity's raycast returns the collider GameObject instead of the topmost object in the hierarchy. This means that to figure out whether the object is selectable, we have to go upwards through the hierarchy to find out if the RUISSelectable script can be found on any of the collider object's ancestors. If any of the ancestors have this script, the object is selectable. If the RUISSelectable script cannot be found anywhere in this hierarchy, the object should be ignored.

Once an object has been marked as selectable, we highlight it visually to show the user that the object can be interacted with. I will explain this more thoroughly soon, when detailing the RUISSelectable implementation details. After highlighting the object as a selectable object, we check whether the selection button was actually pressed.

In RUIS we have implemented different styles of selection actions, so the user has some choice when developing his application. The main choice is whether to select by toggle selecting, by tapping the button, or to grab select, by holding the button down. In addition to this, we have implemented the possibility to press the selection button down when nothing is highlighted and sweeping over the object to be selected, which helps in selecting small objects.

The difference between toggle and grab selection comes into play when the selection button gets released. If grab selection is specified, the button release event should also release the selection, while for toggle events a second button press is required. For toggle events the first button release therefore just sets a local variable that keeps track that the object can actually be released. Once a second selection button release is detected, the object is let go. 

For skeleton wand selection this dependency on press and release events poses an interesting question. What if the selection method is not a button, but a triggered event that does not have an intuitive press and release signal? For this functionality we have included the IsSelectionButtonStandard function in RUISWand. By querying this function RUISWandSelector can check whether the selection functions are actually fired by the press of a real button, or just emulated with for example gestures. If the selection button is found to be non-standard, RUISWandSelector can just refrain from using the SelectionButtonWasReleased function and rely only on the triggered press event. This concludes the wand side of the selection process.

To mark an object as selectable, the object needs to have the RUISSelectable component added to it. This way the RUISWandSelector script is able to highlight and select the object. While the selection itself is handled on the wand's end, the effects of both highlight and selection are done for each selectable in the RUISSelectable script. These effects are showing the highlight visually and handling the movement of the object. 

To show the highlight graphically, some modification will have to be made to the materials of the object. We decided to take the least invasive route and add a new highlight material on top of the material stack of the object. The material used for this can be specified by the user, but as a default we use a translucent green material. When the wand selector sends the highlight start event to the RUISSelectable script, we add the specified material to the end of the material list. Conversely, at the end of the highlight we remove the last element of the material list. For this method to work, no other script should be able to modify the material list of the object. This method also requires the user to create the material specifications, mainly which material affects which part of the model, in a way that shows the highlight properly. In many cases models downloaded off the web will not work directly with the highlight system - the last material in the stack might for example only affect the legs of a chair instead of the whole model. In RUIS we use the same method to also display object selection. In that case the object is shown by default with a translucent red material overlaid on top.

To handle the movements of the object, two cases will have to be considered: what the object does when it is selected and how it will behave once the selection has ended. During the selection, we make the possible Rigidbody of the object kinematic and start setting the position and rotation of the object ourselves. Once the selection ends, the Rigidbody will be set to its original state. After this, if the Rigidbody is non-kinematic, the object can be specified to maintain its momentum, making it continue in the direction of the movement before the selection release. This is handy for throwing objects around.

In RUIS we provide the user with a selection of different selection movement methods, both for positional and rotational movement. Both offer the same four choices: snap to wand, relative to wand, along selection ray and do not grab. Of these, the do not grab option is the most straightforward, as it does not do anything to the position or rotation. The others have different meanings dependent on the context, whether it is position or rotation.

The snap to wand choice for positional movement snaps the selected object straight to the location of the wand in the game world. Similarly, when used for rotational movement the rotation of the selected object will match the wand's rotation perfectly. This type of interaction is used for example when picking up objects for closer inspection. Quite close to the snap to wand behavior is the relative to wand counterpart. Relative to wand starts by marking the position and rotation of the wand at the start of the selection and moving the object in relation to the wand's difference from this starting transform. This type of behavior is useful when precisely positioning objects in a virtual world, as the original transform of the object will stay the same until the user moves the wand. Finally, the along selection ray setting gives the user the option to move around objects as if they were at the end of a long stick. To implement this feature we save the original distance of the object to the selection ray start and keep this distance constant during the selection. The object is positioned each frame to be at this distance from the selection ray start and in the direction specified by the ray. Alternatively the user can specify which distance he wants the object to be snapped to. The rotation of the object is set with the LookAt function, making the object face the direction of the selection ray. In practice, this is the same as setting the rotation of the object to match the direction of the ray, like in the snap to wand option. Using this method of movement is appropriate for example for moving objects for large distances. As mentioned earlier, all these movement types can be mixed, so that one method is used for positioning and one for rotational movement. For example, for completely precise positioning, it might be best to have positional movement set to be relative to wand movements, while the rotational movement is snapped directly to the wand.

Once the user releases the selection, it might be logical to have the object maintain its momentum if it was a Rigidbody. We do this by buffering the velocities of the object while it is selected. On selection release, these velocities are then averaged and the object velocity is set directly. For angular velocity, the selectable object references the angular velocity given by the selector, while the selector just gets this information directly from the wand. This way the angular velocity can be as accurate as possible.

\subsection{Head Tracking}
\label{subsection:wandcontrollers:headtracking}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Evaluation}
\label{chapter:evaluation}

\section{Final Toolkit}
\label{section:finaltoolkit}

\section{RUIS on the T-111.5400 Virtual Reality P Course}
\label{section:vrcourse}

Each Spring the Department of Media Technology organizes the course T-111.5400 Virtual Reality P. The purpose of the course is to familiarize students with the challenges inherent in designing virtual reality applications. The earlier iterations of this course have used the Processing version of RUIS, but this Spring the course migrated to the Unity3D version. 

\hl{some data from course questionnaire? comparison with old versions of the course}

\section{TurboTuscany}
\label{section:turbotuscany}

For the launch of the RUIS we decided to create a demo using the Oculus Rift Tuscany demo as a base. This way, we can showcase the features of RUIS, while still creating a demo that is instantly recognizable to most Oculus Rift owners. The development of TurboTuscany also revealed many things that we could improve upon and helped us focus our development efforts on the most important features still missing or unfinished, e.g. head tracking and Kinect avatar customisability.

The original Oculus Tuscany demo features a scene in Tuscany that the user can explore by using theh Rift and a controller or keyboard. The most noticeable new feature we add to this demo is a user avatar. The user can see his own tracked body, or in this case a grandma avatar, when looking around with the Rift. Using his own body the user can then interact with the environment in many ways, such as walking, throwing objects and falling off rooftops. At the time of release, our TurboTuscany demo was the only Oculus Rift demo featuring thorough Kinect avatar functionality like this. Movement blending between the default avatar animations and Kinect are handled by our blending system. The grandma avatar's legs are animated via Mecanim when the character is moving and by Kinect when the grandma is standing still. The rest is completely controlled by Kinect. This results in more realistic looking walking, since the character does not slide against the ground.

In addition to the Kinect features mentioned, we add PS Move and Razer Hydra functionality to provide for example positional head tracking, a feature Oculus Rift sorely misses to be at its best in virtual reality applications. The details of the head tracking system itself are covered in section \ref{subsection:wandcontrollers:headtracking}. Both controllers are also used to interact with objects with the selection methods outlined in section \ref{subsection:wandcontrollers:selection}.

On top of all of this, we added many fun ways to interact with the environment and play with the physics. The user can use his avatar to climb up ladders with his hands, hit balls with a baseball bat, open and close window shutters or just run around the environment throwing furniture around. All of this adds to a quite comprehensive demonstration of the capabilities of RUIS.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Conclusions}
\label{chapter:conclusions}


% Load the bibliographic references
% ------------------------------------------------------------------
% You can use several .bib files:
% \bibliography{thesis_sources,ietf_sources}
\bibliography{ref}


% Appendices go here
% ------------------------------------------------------------------
% If you do not have appendices, comment out the following lines
%\appendix
% \input{appendices.tex}

%\chapter{First appendix}
%\label{chapter:first-appendix}

%This is the first appendix. You could put some test images or verbose data in an
%appendix, if there is too much data to fit in the actual text nicely.

%For now, the Aalto logo variants are shown in Figure~\ref{fig:aaltologo}.

%\begin{figure}
%\begin{center}
%\subfigure[In English]{\includegraphics[width=.8\textwidth]{aalto-logo-en}}
%\subfigure[Suomeksi]{\includegraphics[width=.8\textwidth]{aalto-logo-fi}}
%\subfigure[Pä svenska]{\includegraphics[width=.8\textwidth]{aalto-logo-se}}
%\caption{Aalto logo variants}
%\label{fig:aaltologo}
%\end{center}
%\end{figure}


% End of document!
% ------------------------------------------------------------------
% The LastPage package automatically places a label on the last page.
% That works better than placing a label here manually, because the
% label might not go to the actual last page, if LaTeX needs to place
% floats (that is, figures, tables, and such) to the end of the
% document.
\end{document}
